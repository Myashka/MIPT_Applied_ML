# Advanced course of Machine learning at MIPT.

[Открытый курс по ML от ФПМИ МФТИ](https://github.com/girafe-ai/ml-course)
Были изучены материалы курса, законспектированы лекции, выполнены лабораторные работы. Ниже приведено описание проделанной работы.

## My homeworks: 
|Тип|Название|Описание|
|---|--------|--------|
|lab|[NLP](https://github.com/Myashka/MIPT_Applied_ML/tree/main/Homeworks/Lab1_NLP)| Лабораторная работа состояла из 2-ух частей.<br>1. В [первой части](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/Lab1_NLP/Lab1_NLP_part1_Embedding_based_MT.ipynb) была использована предобученная модель для слов русского и украинского языков. С помощью линейной регрессии было произведено embedding space mapping.<br>Для улучшения качетсва было произведено ортогоналное преобразование векторных пространств на основе SVD. На основе этого было выполнено Unsupervised MT.<br>2. Во [второй части](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/NMT/Lab1_NLP_part2_NMT.ipynb) решалась задача нейромашинного перевода. Была произведена обычная работа по препроцессингу текстовых данных. Протестированы разные Encoder-Decoder архитектуры:<br>1) Простой LSTM encoder-decoder;<br>2) CNN-encoder LSTM decoder;<br>3) CNN-encoder с positional encoding, что ожидаемо не принесло улучшения в качество модели;<br>4) Самую лучшую метрику показала biLSTM модель на основе GloVe весов с механихмом attention.<br>Трекались и отслеживались модели в _tensorboard_.|
|lab|[RL](https://github.com/Myashka/MIPT_Applied_ML/tree/main/Homeworks/Lab2_RL)| Был реализован Q-learning approximate agent для игры в Atari Breakout с интерфейсов gym ([ссылка](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/Lab2_RL/Lab2_Atari_DQN.ipynb)).<br>Для обработки изображения использовался frame-buffer для отслеживания движения шарика. Использовался replay buffer для ускорения обучения модели. Сам агент представляет собой свертучную сеть.<br>Для препроцессинга изображение было обрезано и сделан перевес в сторону крассного канала.<br>Во время обучения также использовалась Target модель для того чтобы процесс авторегресси был более стабильным, она обнавлялась каждый 5000 шагов.<br>Обучалась модель с advantage (_dueling DQN_) и без, первая показала score в 53 поинта в среднем за 5 игр. Тренировалась и та и другая около 3-ёх дней.|
|assignment|[Word vectors](https://github.com/Myashka/MIPT_Applied_ML/tree/main/Homeworks/assignment1_01_Word_Vectors)| Были имплементированы BoW и TF-IDF ([файл](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/assignment1_01_Word_Vectors/features.py)) Оба способа векторизации были применены для бинарной классификации текстов, toxic-or-not. TF-IDF показал больший ROC_AUR на тесте, так как он лучше учитывает важность слов. Было произведено сравнение с Naive-Bias и использован GloVe из gensim, они показали результаты хуже, чем первые два метода ([ноутюук](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/assignment1_01_Word_Vectors/assignment1_01_Word_Vectors.ipynb)).|
|assignment|[Three headed network](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_02_cnn_for_texts.ipynb)| Это задание было выполнено в ноутбуке с практическим занятием, где использовалось несколько входов нейронной сети для предсказания З/П по информации о резюме.|
|assignment|[Fine-tuning](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_05_bert_for_text_classification.ipynb)| Это задание было выполнено в ноутбуке с практическим занятием, где BERT был fine-tuned для классификации текстов.|
|assignment|[Conv VAE](https://github.com/Myashka/MIPT_Applied_ML/blob/main/Homeworks/assignment1_04_conv_cvae/assignment1_04_convolutional_cvae.ipynb)| Была реализована Conv VAE для CIFAR10. Латентное пространство получилось более однородное.|

## Seminars and practics:

Неделя| Семинар | Описание |
|-----|---------|----------|
|  01_1 |[Word embeddings](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_01_word_embeddings.ipynb) | Использована библиотека _nltk_ для предобработки текстовых данных. Визуализированны word emgeddings с помощью UMAP и gensim Word2Vec. С помощью _kMeans_ кластеризованы некоторые фразю. Обучена собственный _Word2Vec_ модель.|
|  02_1 |[CNN for texts](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_02_cnn_for_texts.ipynb) | Выполнена стандартная предобработка для текстовых данных. В итоге самостоятельно была создана модель с тремя головами для пресказания З/П по информации из резюме. Для считывания резличных данных были использованы CNN для коротких текстовых данных, и LSTM для длинных опиисаний.|
|  03_1 |[Seq2seq for NMT](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_03_seq2seq_for_nmt.ipynb) | Для задачи _NMT_ была создана архитектура Encoder-Decoder. Предобработаны текстовые данные. Обучена модель для перевода с французского на английский Логирование производилось tensorboad'ом.|
|  03_2 |[Attention](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_03_2_attention_basics_and_tensorboard.ipynb) | На синтатических данных был посчитан _attention_.|
|  04_1 |[BiLSTM for PoS Tagging](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_04_bilstm_for_pos_tagging.ipynb) | Создана двуслойная biLSTM модель на основе glove embeddings для определения частей речи предложения.<br>Была также сделана biLSTM модель на уровне символов, выходы которой подавались вместе с embedding'ами GloVe в следующую biLSTM, но это не дало прироста в качестве.|
|  04_2 |[Positional encoding](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_04_positional_encoding_2022.ipynb) | Разобраны формулы PE, явно визуализированы различия при различных частотах и измерениях данных.|
|  05_1 |[BERT](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_05_bert_for_text_classification.ipynb) | Был использован BERT для классификации текста, fine-tuned логистической регрессией. Также использовано api _Transformers_.|
|  06_1 |[Question Answering](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_06_question_answering_and_tts.ipynb) | Был использован готовый Fine-tuned BERT для QA. Также был протестирован RuBERT. Также была выполнена задача Text2Speech для английского языка с помощью Tacotron 2. Для русского языка использовалась готовая обученная модель Silero.|
|  07_1 |[GYM and CE](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_07_gym_and_crossentropy_method.ipynb) | Знакомство с интерфейсом gym. Реализация метода CrossEntropy для обучения агента в CartPole.<br>Самостоятельная реализация Deep CE метода для _LunarLander_, использование joblib для ускорения обучения модели.|
|  08_1 |[Q-learning](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_08_qlearning.ipynb) | Реализован Q-learning agent на основании уравнений Беллмана для решения Taxi. Для решения CartPole состояния были дискретезированы. |
|  09_1 |[Appoximate Q-learning](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_09_approximate_q_learning.ipynb) | Был написан Agent, который аппроксимирует Q-функцию для выбора действия и улучшения политики Решена задача CartPole. |
|  10_1 |[Reinforce](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_10_reinforce.ipynb) | Был реализован _REINFORCE_ алгоритм для апроксимации политики агента (использован log-derivative trick).<br>Самостоятельно был реализован A2C агент для решения Acro-Bot. После нескольких часов обучения и нескольких запуском модель чему-то научилась. Внутри агента отдельно учились две сети, для ценности действия и в текущий момент и ценности состояния.|
|  11_1 |[RL for Seq2seq](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_11_rl_for_seq2sec.ipynb) | Сначала была обучена модель для транскрипции иврита на английский обычными методами, при помощи CrossEntropy loss. После чего была напрямую дообучена на целевую метрику (расстояние Левенштейна).|
|  12_1 |[Style Transfer](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_12_neural_style_tutorial.ipynb) | Был произведен Style transfer с одного изборажения на другое. За счет того, карты признаков на разной глубине обращают внимание на разные вещи, то за счёт этого можно градиентными методами подобнать одно изображение под другое.|
|  13_1 |[Knowledge distillation](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_13_knowledge_distillation.ipynb) | Была создана сверточная модель для классификации над CIFAR-10. На эти же данные была обучена меньшая модель, а также дистилирована другая модель. Вторая показала лучшие рещультаты. Ошибка рассчитывалась при помощи KL-дивергенции.|
|  14_1 |[Semantic segmentation](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_14_semantic_segmentation.ipynb) | Была создана U-net подобная модель (вместо UpPooling использовалась Transposed Conv). Была обучена модель с предобученным backbone сетью resnet. Во втором случае, как и ожидалось, получили лучшее качество. Для аугментаций использовалась библиотека _Albumentations_. Трекалась модель в W&B.|
|  15_1 |[VAE](https://github.com/Myashka/MIPT_Applied_ML/blob/main/week_15_vae.ipynb) | Была реализована VAE модель для датасета CIFAR10. KL-Loss считался между нормальным стандартным распределением и тем, что сэмплила модель. Для прокидывания градиентов использовался Reparameterization trick.|
