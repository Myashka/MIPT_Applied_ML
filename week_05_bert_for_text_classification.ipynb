{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izA3-6kffbdT"
   },
   "source": [
    "# Practice: A Visual Notebook to Using BERT for the First Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEBgv15zoaX6"
   },
   "source": [
    "*Credits: first part of this notebook is strongly based on Jay Alammar's [great blog post](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/). His blog is a great way to dive into the DL and NLP concepts.*\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVgtANpYoaX7"
   },
   "source": [
    "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject of \"negatively\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oCi6ZSnoaX7"
   },
   "source": [
    "## Models: Sentence Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyRwVEI4oaX7"
   },
   "source": [
    "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
    "\n",
    "Under the hood, the model is actually made up of two model.\n",
    "\n",
    "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
    "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
    "\n",
    "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQVOYe4PoaX8"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S7DFxaeoaX9"
   },
   "source": [
    "The dataset we will use in this example is [SST2](https://nlp.stanford.edu/sentiment/index.html), which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\n",
    "\n",
    "\n",
    "<table class=\"features-table\">\n",
    "  <tr>\n",
    "    <th class=\"mdc-text-light-green-600\">\n",
    "    sentence\n",
    "    </th>\n",
    "    <th class=\"mdc-text-purple-600\">\n",
    "    label\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      apparently reassembled from the cutting room floor of any given daytime soap\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      they presume their audience won't sit still for a sociology lesson\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPMv-7fOoaX-"
   },
   "source": [
    "## Installing the transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9zZk1UgoaX_"
   },
   "source": [
    "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "To9ENLU90WGl"
   },
   "outputs": [],
   "source": [
    "#!pip install -Uqq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Part 1. Using BERT for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwbK19NFoaYB"
   },
   "source": [
    "## Loading pretrained BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GITfp914oaYB"
   },
   "source": [
    "Here we will be using the pretrained DistilBERT model from `transformers` library. The easiest way to use such model is to use a `pipeline`. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4LyOoG8IoaYB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.052928488701581955,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role',\n",
       "  'sequence': \"hello i'm a role model.\"},\n",
       " {'score': 0.0396859385073185,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i'm a fashion model.\"},\n",
       " {'score': 0.03474380820989609,\n",
       "  'token': 2449,\n",
       "  'token_str': 'business',\n",
       "  'sequence': \"hello i'm a business model.\"},\n",
       " {'score': 0.03462299332022667,\n",
       "  'token': 2944,\n",
       "  'token_str': 'model',\n",
       "  'sequence': \"hello i'm a model model.\"},\n",
       " {'score': 0.018145199865102768,\n",
       "  'token': 11643,\n",
       "  'token_str': 'modeling',\n",
       "  'sequence': \"hello i'm a modeling model.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "unmasker = pipeline('fill-mask', 'distilbert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnA23_CpoaYC"
   },
   "source": [
    "However, such approach is not very flexible and certainly doesn't allow you to fine-tune a model. For this reason we will use the model in a more manual way. For this we load the model and appropriate tokenizer and use them together. Here is how we can use them to extract features from our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O8gHVTAEoaYC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, logging\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()  # Ignore warning on model loading.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "text = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.'\n",
    "tokenized_text = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**tokenized_text)\n",
    "\n",
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5DAZw0tN8wi"
   },
   "source": [
    "Except for the `logging` part, everything looks very similar to the code we saw in previous practice notebooks. The first thing we do is, just like always, tokenize our text.\n",
    "\n",
    "> **Note:** as you can see, we used `return_tensors` keyword argument in code above. This parameters just tells tokenizer to convert result into a PyTorch tensors to use them with our model. If we don't specify this parameter, we will get exactly same results, but packed into a python `list` objects.\n",
    "\n",
    "Let's look at this step a little closer. What exactly does the `tokenizer.__call__` return? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KH77JkiNPCxA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: list[int], length 23: [101, 19544, 2213, 12997, 17421, ...]\n",
      "attention_mask: list[int], length 23: [1, 1, 1, 1, 1, ...]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(text)\n",
    "\n",
    "for key, values in tokenized_text.items():\n",
    "    values_type = type(values).__name__\n",
    "    item_type = type(values[0]).__name__\n",
    "    values_sample = f\"[{', '.join(str(value) for value in values[:5])}, ...]\"\n",
    "    print(f\"{key}: {values_type}[{item_type}], length {len(values)}: {values_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzDnPSvrQdCB"
   },
   "source": [
    "The contents may differ for different models, however for the `DistilBert` model tokenizer returns a `dict`-like object with two python lists under keys `\"input_ids\"` and `\"attention_mask\"`. Both lists have the same length and the attention mask seems to only have ones. We'll deal with the mask later, for now let's focus on `\"input_ids\"` which is the token ids for the tokenized sequence. Let's decode them to make sure and see what is actually going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SK-Yg-FDMN3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'lore', '##m', 'ip', '##sum', 'do', '##lor', 'sit', 'am', '##et', ',', 'con', '##se', '##ct', '##et', '##ur', 'adi', '##pis', '##cing', 'eli', '##t', '.', '[SEP]']\n",
      "Decoded sequence: '[CLS] lorem ipsum dolor sit amet, consectetur adipiscing elit. [SEP]'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])}\")\n",
    "print(f\"Decoded sequence: '{tokenizer.decode(tokenized_text['input_ids'])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syJ8naWFT87_"
   },
   "source": [
    "We see that tokenizer actually does quite a lot of work behind the curtains: it lowercases the sequence (remember, we use `*-uncased` model, which implies that it doesn't understand the upper case), adds special tokens (`[CLS]` and `[SEP]`) and applies the BPE. That is how we get 23 tokens for such a little text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUmYkLeHd1m7"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVlcZqXqoaYC"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3YBY3DWoaYC"
   },
   "source": [
    "However, working with manually edited sentence is not interesting. Let's use our model to work with a dataset for sentiment classification. We'll use pandas to read the dataset and load it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cyoj29J24hPX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker 's bartleby should have been t...      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_url = (\n",
    "    'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    ")\n",
    "dataset = pd.read_csv(dataset_url, delimiter='\\t', header=None)\n",
    "dataset.columns = ['text', 'label']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMVE3waNhuNj"
   },
   "source": [
    "For performance reasons, we'll only use 2,000 sentences from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [],
   "source": [
    "dataset = dataset[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRc2L89hh1Tf"
   },
   "source": [
    "We can ask pandas how many sentences are labeled as \"positive\" (value 1) and how many are labeled \"negative\" (having the value 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jGvcfcCP5xpZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1041\n",
       "0     959\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZDBMn3wiSX6"
   },
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiNzCErkoaYE"
   },
   "source": [
    "Before we can hand our sentences to BERT, we need to so some processing to put them in the format it requires. First, let's split our `dataset` into separate `texts` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m8ipGP0raY7r"
   },
   "outputs": [],
   "source": [
    "texts = dataset['text'].tolist()\n",
    "labels = dataset['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmNUir6fcLbl"
   },
   "source": [
    "Now we need to tokenize our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PyLowtCxajkq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: list[list], length 2000\n",
      "attention_mask: list[list], length 2000\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Tokenize the texts in dataset.\n",
    "# Hint: our tokenizer can also work with lists of strings.\n",
    "# tokenized_texts = ...\n",
    "tokenized_texts = tokenizer(texts)\n",
    "\n",
    "for key, values in tokenized_texts.items():\n",
    "    values_type = type(values).__name__\n",
    "    item_type = type(values[0]).__name__\n",
    "    print(f\"{key}: {values_type}[{item_type}], length {len(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooqGLMl7cTe-"
   },
   "source": [
    "We obtained a list of lists. However, what we want is `torch.tensor` so that we could use it with our model. It's time to remember how we used to specify the `return_tensors` option! However, if we were to just specify it blindly, we would get an error. The problem here lies in the fact that sequences tend to have different lenghts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7SMycKghdkh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "16\n",
      "45\n",
      "22\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "for seq in tokenized_texts[\"input_ids\"][:5]:\n",
    "    print(len(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MjmmfMseJVf"
   },
   "source": [
    "The most common solution to this problem, often used in NLP is the use of padding. Luckily for us, tokenizer from `transformers` can do the padding for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ONgnhkuxe9kB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: Tensor, torch.Size([2000, 59])\n",
      "attention_mask: Tensor, torch.Size([2000, 59])\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = tokenizer(dataset['text'].tolist(), return_tensors=\"pt\", padding=True)\n",
    "\n",
    "for key, values in tokenized_texts.items():\n",
    "    values_type = type(values).__name__\n",
    "    print(f\"{key}: {values_type}, {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1AlfPcefj-m"
   },
   "source": [
    "However, we just added lot's of extra items into most of our sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XjfXDg8pgsFd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1037, 18385,  ...,     0,     0,     0],\n",
       "        [  101,  4593,  2128,  ...,     0,     0,     0],\n",
       "        [  101,  2027,  3653,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2023,  2028,  ...,     0,     0,     0],\n",
       "        [  101,  1999,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  1996,  3185,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkhDolkRgrti"
   },
   "source": [
    "Note how all the sequences end with zeros. We already encountered such a problem, when trained our RNN on previous lessons and we tackled it with specifying the padding index to the `CrossEntropyLoss` so that our model doesn't train to predict padding. Right now we don't want to train our model to do anything, however, we are working with a transformer, which means that it uses the self-attention operation extensively. If we were to simply add these extra items, it would be likely to affect our results. And this is exactly the place where the `\"attention_mask\"` comes into play. It is used exactly to mask the padding from getting in a way of attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gSCXbXVLg_7T"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.pcolormesh(tokenized_texts[\"attention_mask\"])\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK-CQB9-kN99"
   },
   "source": [
    "## And Now, Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0UsQGQ2oaYF"
   },
   "source": [
    "Now that we have our model and inputs ready, let's run our model! However, running it on a cpu takes several minutes. We can speed this up via using the GPU. For this, however, we would need to split our dataset into batches of data.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBqHs4d_ObTc"
   },
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model. Also remember, how we created the `labels` variable to hold our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "39UVjAV56PJz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 768)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        texts_batch = tokenized_texts[\"input_ids\"][i : i + batch_size].to(device)\n",
    "        masks_batch = tokenized_texts[\"attention_mask\"][i : i + batch_size].to(device)\n",
    "        output = model(texts_batch, masks_batch)\n",
    "        batch_features = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        features.append(batch_features)\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02wlGPSLoaYG"
   },
   "source": [
    "Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9bhSJpcv1Bl"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW_IiKvToaYG"
   },
   "source": [
    "We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyEwr7yYD3Ci"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# [EXTRA] Grid search for parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCT9u8vAwnID"
   },
   "source": [
    "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the best hyperparameter values into the model declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gG-EVWx4CzBc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')  # Ignore warning on model fitting.\n",
    "lr_clf = LogisticRegression().fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rUMKuVgwzkY"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n",
    "\n",
    "So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iCoyxRJ7ECTA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xA5YIJPDYek"
   },
   "source": [
    "Another way to evaluate classification model is to plot the ROC curve and compute the area under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "186Bwg7UDLkU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "proba = lr_clf.predict_proba(train_features)[:, 1]\n",
    "auc = roc_auc_score(train_labels, proba)\n",
    "plt.plot(*roc_curve(train_labels, proba)[:2], label=f'train AUC={auc:.4f}')\n",
    "\n",
    "proba = lr_clf.predict_proba(test_features)[:, 1]\n",
    "auc = roc_auc_score(test_labels, proba)\n",
    "plt.plot(*roc_curve(test_labels, proba)[:2], label=f'test AUC={auc:.4f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75oyhr3VxHoE"
   },
   "source": [
    "How good is this score? What can we compare it against? Let's first look at a dummy classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "lnwgmqNG7i5l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.514 (+/- 0.003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = DummyClassifier()\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(f\"Dummy classifier score: {scores.mean():.3f} (+/- {2 * scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lg4LOpoxSOR"
   },
   "source": [
    "So our model clearly does better than a dummy classifier. But how does it compare against the best models?\n",
    "\n",
    "For reference, the [highest accuracy score](http://nlpprogress.com/english/sentiment_analysis.html) for this dataset is currently **96.8**. DistilBERT can be trained to improve its score on this task – a process called **fine-tuning** which updates BERT’s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of **90.7**. The full size BERT model achieves **94.9**.\n",
    "\n",
    "And that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at [fine-tuning](https://huggingface.co/transformers/examples.html#glue). You can also go back and switch from distilBERT to BERT and see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJQuqV6cnWQu",
    "outputId": "402d109c-01bb-485d-a510-4be8684c9c06"
   },
   "source": [
    "## Part 2: Looking back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIBbP_oroaYH",
    "outputId": "402d109c-01bb-485d-a510-4be8684c9c06"
   },
   "source": [
    "Now it is your turn to reproduce the steps above.\n",
    "\n",
    "We shall revisit the first homework and see whether we could improve the results a little bit more. The average ROC-AUC on test set was around $0.9$ (using the words embeddings). \n",
    "\n",
    "__Let's see whether we can beat it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kz8QBEXozHJx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>should_ban</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The picture on the article is not of the actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Its madness. Shes of Chinese heritage, but JAP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Fuck You. Why don't you suck a turd out of my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>God is dead\\nI don't mean to startle anyone bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>THIS USER IS A PLANT FROM BRUCE PERENS AND GRO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   should_ban                                       comment_text\n",
       "0           0  The picture on the article is not of the actor...\n",
       "1           1  Its madness. Shes of Chinese heritage, but JAP...\n",
       "2           1  Fuck You. Why don't you suck a turd out of my ...\n",
       "3           1  God is dead\\nI don't mean to startle anyone bu...\n",
       "4           1  THIS USER IS A PLANT FROM BRUCE PERENS AND GRO..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_url = 'https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/comments_small_dataset/comments.tsv'\n",
    "dataset = pd.read_csv(dataset_url, sep='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPyxeBCgGtfg"
   },
   "source": [
    "One last note: this dataset contains some very long sentences, while the vast majority of sequences fall into category of 500 tokens and less:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "XdZjuVxRoaYH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = dataset[\"comment_text\"].tolist()\n",
    "tokenized_texts = tokenizer(texts)\n",
    "ids_lens = list(len(toks) for toks in tokenized_texts[\"input_ids\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ids_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66wufVncHF8f"
   },
   "source": [
    "We already know, how to tackle the problem of different sizes of sequences with padding. However, blind padding here would make pad all the sequenes to the size of the largest one, which seems to be an overkill. In such case it might be sensible to actually truncate the too-long sequences into a fixed length, say 512. And we can do this easily by specifying the `max_length` and `truncation=True` arguments to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_LOYXhz_FKTr"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "MAX_LENGTH = 512\n",
    "tokenized_texts = tokenizer(dataset['comment_text'].tolist(), return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_texts.input_ids.shape\n",
    "\n",
    "plt.pcolormesh(tokenized_texts[\"attention_mask\"])\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = tokenizer(dataset['comment_text'].tolist(),\n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,\n",
    "                            truncation=True,\n",
    "                            max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_texts.input_ids.shape\n",
    "\n",
    "plt.pcolormesh(tokenized_texts[\"attention_mask\"])\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"should_ban\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        texts_batch = tokenized_texts[\"input_ids\"][i : i + batch_size].to(device)\n",
    "        masks_batch = tokenized_texts[\"attention_mask\"][i : i + batch_size].to(device)\n",
    "        output = model(texts_batch, masks_batch)\n",
    "        batch_features = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        features.append(batch_features)\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression().fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9653333333333334"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc1hBVfbzHJ7"
   },
   "source": [
    "So, how does it look? Did we achieve better results? \n",
    "\n",
    "Here come some further ideas:\n",
    "\n",
    "* Try using the larger BERT (e.g. BERT-base or BERT-large) and compare the results (be careful, they require more memory).\n",
    "\n",
    "* Using BERT output for translation? Why not ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "practice_bert_for_text_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
