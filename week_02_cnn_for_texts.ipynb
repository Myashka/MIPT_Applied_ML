{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13pL--6rycN3"
   },
   "source": [
    "# Practice: Dealing with texts using CNN\n",
    "_Reference: Based on YSDA [materials](https://github.com/yandexdataschool/nlp_course/blob/master/week02_classification/seminar.ipynb). Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the core assignment idea._\n",
    "\n",
    "Today we're gonna apply the newly learned tools for the task of predicting job salary.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34x92vWQycN_"
   },
   "source": [
    "## About the challenge\n",
    "For starters, let's download and unpack the data.\n",
    "\n",
    "You can also get it from [Yandex.Disk](https://yadi.sk/d/vVEOWPFY3NruT7) or [the competition page](https://www.kaggle.com/c/job-salary-prediction/data) (pick `Train_rev1.*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vwN72gd4ycOA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   145    0   145    0     0    204      0 --:--:-- --:--:-- --:--:--   204\n",
      "\n",
      "100   342  100   342    0     0    282      0  0:00:01  0:00:01 --:--:--   282\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  5  119M    5 7008k    0     0  2119k      0  0:00:57  0:00:03  0:00:54 3505k\n",
      " 13  119M   13 16.5M    0     0  3930k      0  0:00:31  0:00:04  0:00:27 5644k\n",
      " 20  119M   20 24.5M    0     0  4738k      0  0:00:25  0:00:05  0:00:20 6287k\n",
      " 27  119M   27 32.5M    0     0  5275k      0  0:00:23  0:00:06  0:00:17 6656k\n",
      " 34  119M   34 40.8M    0     0  5727k      0  0:00:21  0:00:07  0:00:14 8586k\n",
      " 43  119M   43 52.0M    0     0  6407k      0  0:00:19  0:00:08  0:00:11 9236k\n",
      " 50  119M   50 59.9M    0     0  6601k      0  0:00:18  0:00:09  0:00:09 8901k\n",
      " 57  119M   57 68.5M    0     0  6811k      0  0:00:17  0:00:10  0:00:07 9010k\n",
      " 63  119M   63 76.2M    0     0  6909k      0  0:00:17  0:00:11  0:00:06 8970k\n",
      " 71  119M   71 85.4M    0     0  7105k      0  0:00:17  0:00:12  0:00:05 9117k\n",
      " 76  119M   76 91.4M    0     0  7003k      0  0:00:17  0:00:13  0:00:04 7984k\n",
      " 84  119M   84  100M    0     0  7212k      0  0:00:16  0:00:14  0:00:02 8348k\n",
      " 90  119M   90  108M    0     0  7266k      0  0:00:16  0:00:15  0:00:01 8207k\n",
      " 97  119M   97  117M    0     0  7352k      0  0:00:16  0:00:16 --:--:-- 8355k\n",
      "100  119M  100  119M    0     0  7340k      0  0:00:16  0:00:16 --:--:-- 8004k\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=1\" -o Train_rev1.csv.tar.gz\n",
    "!tar xzf Train_rev1.csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xIjzhbXEpHfT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Train_rev1.csv\", index_col=None)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7kznuJfycOH"
   },
   "source": [
    "One problem with salary prediction is that it's oddly distributed: there are many people who are paid standard salaries and a few that get tons o money. The distribution is fat-tailed on the right side, which is inconvenient for MSE minimization.\n",
    "\n",
    "There are several techniques to combat this: using a different loss function, predicting log-target instead of raw target or even replacing targets with their percentiles among all salaries in the training set. We gonna use logarithm for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UuuKIKfrycOH"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data[\"Log1pSalary\"] = np.log1p(data[\"SalaryNormalized\"]).astype(\"float32\")\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data[\"Log1pSalary\"], bins=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fcu-qmHRycOK"
   },
   "source": [
    "Our task is to predict one number, __Log1pSalary__.\n",
    "\n",
    "To do so, our model can access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p9vyA_erycOK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220198</th>\n",
       "      <td>72346413</td>\n",
       "      <td>Technical Manager / Head of Development  NET /...</td>\n",
       "      <td>Job Title: Technical Manager / Head of Develop...</td>\n",
       "      <td>Manchester Lancashire North West</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Applause IT Innovation</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>35000 - 45000 per annum + Benefits</td>\n",
       "      <td>40000</td>\n",
       "      <td>cwjobs.co.uk</td>\n",
       "      <td>10.596660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136495</th>\n",
       "      <td>70331893</td>\n",
       "      <td>Constructive Supervisors</td>\n",
       "      <td>Introduction We are currently looking for a nu...</td>\n",
       "      <td>Rosyth</td>\n",
       "      <td>Rosyth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Matchtech Group plc.</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>22/hr</td>\n",
       "      <td>42240</td>\n",
       "      <td>rengineeringjobs.com</td>\n",
       "      <td>10.651147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111185</th>\n",
       "      <td>69683978</td>\n",
       "      <td>Senior IT Systems Engineer  Fast Growing IT Su...</td>\n",
       "      <td>Senior IT Systems Engineer  Fast Growing IT Su...</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RecruitmentRevolution.com</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>careerbuilder.com</td>\n",
       "      <td>10.596660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "220198  72346413  Technical Manager / Head of Development  NET /...   \n",
       "136495  70331893                           Constructive Supervisors   \n",
       "111185  69683978  Senior IT Systems Engineer  Fast Growing IT Su...   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "220198  Job Title: Technical Manager / Head of Develop...   \n",
       "136495  Introduction We are currently looking for a nu...   \n",
       "111185  Senior IT Systems Engineer  Fast Growing IT Su...   \n",
       "\n",
       "                             LocationRaw LocationNormalized ContractType  \\\n",
       "220198  Manchester Lancashire North West                 UK          NaN   \n",
       "136495                            Rosyth             Rosyth          NaN   \n",
       "111185                         Liverpool          Liverpool    full_time   \n",
       "\n",
       "       ContractTime                    Company          Category  \\\n",
       "220198    permanent     Applause IT Innovation           IT Jobs   \n",
       "136495          NaN       Matchtech Group plc.  Engineering Jobs   \n",
       "111185          NaN  RecruitmentRevolution.com           IT Jobs   \n",
       "\n",
       "                                 SalaryRaw  SalaryNormalized  \\\n",
       "220198  35000 - 45000 per annum + Benefits             40000   \n",
       "136495                               22/hr             42240   \n",
       "111185                               40000             40000   \n",
       "\n",
       "                  SourceName  Log1pSalary  \n",
       "220198          cwjobs.co.uk    10.596660  \n",
       "136495  rengineeringjobs.com    10.651147  \n",
       "111185     careerbuilder.com    10.596660  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna(\"NaN\")\n",
    "data[text_columns] = data[text_columns].fillna(\"NaN\")\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUdclucmycON"
   },
   "source": [
    "## Preprocessing text data\n",
    "\n",
    "Just like last week, applying NLP to a problem begins from tokenization: splitting raw text into sequences of tokens (words, punctuation, etc).\n",
    "\n",
    "__Your task__ is to lowercase and tokenize all texts under `Title` and `FullDescription` columns. Store the tokenized data as a __space-separated__ string of tokens for performance reasons.\n",
    "\n",
    "It's okay to use nltk tokenizers. Assertions were designed for WordPunctTokenizer, slight deviations are okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YzeOxD_aycOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "2         Mathematical Modeller / Simulation Analyst / O...\n",
      "100002    A successful and high achieving specialist sch...\n",
      "200002    Web Designer  HTML, CSS, JavaScript, Photoshop...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw text:\")\n",
    "print(data[\"FullDescription\"][2::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RUWkpd7PycOQ"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def normalize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return ' '.join(tokens)\n",
    "# Lowercase and tokenize titles and descriptions (text_columns).\n",
    "# Store items as space-separated strings of tokens.\n",
    "data[text_columns] = data[text_columns].applymap(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gs-6lnS_ycOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "2         mathematical modeller / simulation analyst / o...\n",
      "100002    a successful and high achieving specialist sch...\n",
      "200002    web designer html , css , javascript , photosh...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized:\")\n",
    "print(data[\"FullDescription\"][2::100000])\n",
    "assert data[\"FullDescription\"][2][:50] == \"mathematical modeller / simulation analyst / opera\"\n",
    "assert data[\"Title\"][54321] == \"international digital account manager ( german )\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouE3L2hyycOX"
   },
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's count how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iC7hBwwjycOX"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Count how many times does each token occur in both \"Title\" and \"FullDescription\".\n",
    "# Build a dictionary { token -> count }.\n",
    "# Hint: you may or may not want to use collections.Counter\n",
    "token_counts = Counter()\n",
    "for row in data[text_columns].values.flatten():\n",
    "    token_counts.update(row.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GiOWbc15ycOb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 202704\n",
      "('and', 2657388)\n",
      "('.', 2523216)\n",
      "(',', 2318606)\n",
      "('the', 2080994)\n",
      "('to', 2019884)\n",
      "...\n",
      "('stephanietraveltraderecruitmnt', 1)\n",
      "('ruabon', 1)\n",
      "('lowehays', 1)\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print(\"\\n\".join(map(str, token_counts.most_common(n=5))))\n",
    "print(\"...\")\n",
    "print(\"\\n\".join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2ih1LV4YhtL"
   },
   "source": [
    "Let's see how many words are there for each count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "nd5v3BNfycOf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(token_counts.values()), range=[0, 10 ** 4], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znuXxeghycOh"
   },
   "source": [
    "Now we will filter tokens a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SeNFBWx5ycOh"
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# We shall only consider tokens that had at least min_count occurences.\n",
    "# Create a list of such tokens.\n",
    "tokens = [token for token, counts in token_counts.items() if counts > min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32454"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "RATIRyPKycOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32456\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + sorted(tokens)\n",
    "print(\"Vocabulary size:\", len(tokens))\n",
    "\n",
    "assert type(tokens) == list\n",
    "assert len(tokens) in range(32000, 35000)\n",
    "assert \"me\" in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqEsgbjZycOo"
   },
   "source": [
    "Build an inverse token index: a dictionary from token to it's index in `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "L60lo1l_ycOq"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "token_to_idx = {token: idx for idx, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "DeAoVo4mycOr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_idx, dict)\n",
    "assert len(token_to_idx) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_idx[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmJAkq3gycOv"
   },
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into neural network-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "JEsLeBjVycOw"
   },
   "outputs": [],
   "source": [
    "UNK_IDX = token_to_idx[UNK]\n",
    "PAD_IDX = token_to_idx[PAD]\n",
    "\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\"Convert a list of tokens into a matrix with padding\"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = [seq.split(\" \") for seq in sequences]\n",
    "\n",
    "    sequences_max_len = max(len(seq) for seq in sequences)\n",
    "    if max_len is None:\n",
    "        max_len = sequences_max_len\n",
    "    else:\n",
    "        max_len = min(sequences_max_len, max_len)\n",
    "\n",
    "    matrix = np.full((len(sequences), max_len), PAD_IDX)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, token in enumerate(seq):\n",
    "            if j >= max_len:\n",
    "                break\n",
    "\n",
    "            matrix[i, j] = token_to_idx.get(token, UNK_IDX)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "JiBlPkdKycOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[10273 28655  2055     1     1]\n",
      " [14296  2700     1     1     1]\n",
      " [26272  9698    14 14484 10270]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines:\")\n",
    "print(\"\\n\".join(data[\"Title\"][::100000].values), end=\"\\n\\n\")\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGOdZ3-dycO4"
   },
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement more advanced encodings: tf-idf, pseudo-time-series, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "pg12nLQijNJg"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "# We only consider top-1k most frequent companies to minimize memory usage\n",
    "company_counts = Counter(data[\"Company\"])\n",
    "top_companies = set(name for name, count in company_counts.most_common(1000))\n",
    "data[\"Company\"] = data[\"Company\"].apply(\n",
    "    lambda company: company if company in top_companies else \"Other\"\n",
    ")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk4jmtAYycO8"
   },
   "source": [
    "## The deep learning part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "TngLcWA0ycO_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  195814\n",
      "Validation size =  48954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "2PXuKgOSycPB"
   },
   "outputs": [],
   "source": [
    "def make_batch(data, max_len=None, word_dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a neural-network-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IDX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
    "    batch[\"Categorical\"] = categorical_vectorizer.transform(\n",
    "        data[categorical_columns].apply(dict, axis=1)\n",
    "    )\n",
    "\n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1.0 - word_dropout)\n",
    "\n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prob, replace_with=UNK_IDX):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prob, 1 - keep_prob])\n",
    "    dropout_mask &= matrix != PAD_IDX\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "I6LpEQf0ycPD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example batch:\n",
      "{'Title': array([[26272, 28401, 31999,     1,     1,     1,     1],\n",
      "       [27772,   187, 18227, 19056, 14801, 22008,  3848],\n",
      "       [10082, 28895, 16876,    31,  8281, 27695,    61]]), 'FullDescription': array([[26272, 28401, 31999, 31307,   909, 26272, 28401, 31999, 15662,\n",
      "        31307],\n",
      "       [27772,   187, 18227, 19056, 14801, 22008,  3848, 24230,   836,\n",
      "           77],\n",
      "       [29212, 20872, 19591,  6092, 15662,  7773, 26125,   909, 28895,\n",
      "        16876]]), 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'Log1pSalary': array([ 9.71154 , 10.463132, 10.71444 ], dtype=float32)}\n",
      "FullDescription shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "example_batch = make_batch(data_train[:3], max_len=10)\n",
    "print(\"Example batch:\")\n",
    "print(example_batch)\n",
    "print(\"FullDescription shape:\", example_batch[\"FullDescription\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "nBfN182zpHfd"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
    "    \"\"\"iterates minibatches of data in random order\"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
    "            target = batch.pop(target_column)\n",
    "            yield batch, target\n",
    "\n",
    "        if not cycle:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQAQPx3-pHfd"
   },
   "source": [
    "To generate minibatches we will use simple pyton generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "fCeb-W9EodjZ"
   },
   "outputs": [],
   "source": [
    "iterator = iterate_minibatches(data_train, 3)\n",
    "batch, target = next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eI5h9UMycPF"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "Our main model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png\" width=600px>\n",
    "\n",
    "This clearly doesn't fit into PyTorch __Sequential__ interface. To build such a network, one will have to use [__PyTorch nn.Module API__](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbhrCEl1pHfc"
   },
   "source": [
    "But to start with let's build the simple model using only the part of the data. Let's create the baseline solution using only the description part (so it should definetely fit into the Sequential model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "2jjPFvWZpHfc"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "# Initially, our FullDescription has a shape [batch_size, seq_len].\n",
    "# After an Embedding layer shape will be [batch_size, seq_len, embedding_size].\n",
    "# However, Conv1d layer expects batches of shape [batch_size, embedding_size, seq_len].\n",
    "# We will use this layer to fix this misunderstanding.\n",
    "class Reorder(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.permute((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3768"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "6LPx2UKepHfd"
   },
   "outputs": [],
   "source": [
    "n_tokens = len(token_to_idx)\n",
    "n_cat_features = len(categorical_vectorizer.vocabulary_)\n",
    "hid_size = 64\n",
    "\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=n_tokens, embedding_dim=hid_size),\n",
    "    Reorder()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMu66aNSpHfd"
   },
   "source": [
    "__Remember!__ We are working with regression problem and predicting only one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CerptbpcpHfd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Check your model. `torch.long` tensors are required for nn.Embedding layers.\n",
    "simple_model(torch.tensor(batch[\"FullDescription\"], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G-n-458pHfd"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "simple_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hUa3gBepHfd"
   },
   "source": [
    "And now simple training pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKiy4otdpHfd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "model = simple_model\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, target in iterate_minibatches(data_train):\n",
    "        # Preprocessing the batch data and target\n",
    "        batch = torch.tensor(batch[\"FullDescription\"], dtype=torch.long, device=device)\n",
    "        target = torch.tensor(target, device=device)\n",
    "\n",
    "        predictions = model(batch)\n",
    "        predictions = predictions.flatten()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Compute loss.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Train with backprop.\n",
    "\n",
    "        history.append(loss.item())\n",
    "        if len(history) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.plot(history, label=\"loss\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOXdOBCnpHfe"
   },
   "source": [
    "To evaluate the model we can switch it to `eval` state. Let's check the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaiQRrfCpHfe"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, data, batch_size=256, name=None, **kwargs):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    data_iterator = iterate_minibatches(data, batch_size, shuffle=False, **kwargs)\n",
    "    for batch, target in data_iterator:\n",
    "        batch = torch.tensor(batch[\"FullDescription\"], dtype=torch.long, device=device)\n",
    "        pred = model(batch)[:, 0].cpu().numpy()\n",
    "\n",
    "        squared_error += np.sum(np.square(pred - target))\n",
    "        abs_error += np.sum(np.abs(pred - target))\n",
    "        num_samples += len(target)\n",
    "\n",
    "    if name is not None:\n",
    "        print(f\"{name}:\")\n",
    "\n",
    "    print(f\"MSE: {squared_error / num_samples:.5f}\")\n",
    "    print(f\"MAE: {abs_error / num_samples:.5f}\")\n",
    "\n",
    "\n",
    "evaluate_model(simple_model, data_train, name=\"Train\")\n",
    "evaluate_model(simple_model, data_val, name=\"Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUUchHCnpHfe"
   },
   "source": [
    "### Bonus area 1: three-headed network.\n",
    "\n",
    "Now you can try to implement the network we've discussed above. Use [__PyTorch nn.Module API__](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOG-2OK2pHfe"
   },
   "outputs": [],
   "source": [
    "class ThreeInputsNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tokens=len(tokens),\n",
    "        n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
    "        hid_size=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_emb = nn.Embedding(n_tokens, hid_size)\n",
    "        # YOUR CODE HERE\n",
    "        # Define modules to process the title.\n",
    "\n",
    "        self.desc_emb = nn.Embedding(n_tokens, hid_size)\n",
    "        # YOUR CODE HERE\n",
    "        # Define modules to process the description.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Define modules to process the categorical features.\n",
    "        # self.category_out = ...\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Define fully-connected part which will take outputs of\n",
    "        # three heads and return the result.\n",
    "        # self.out = ...\n",
    "\n",
    "    def forward(self, whole_input):\n",
    "        input1, input2, input3 = whole_input\n",
    "        title_beg = self.title_emb(input1).permute((0, 2, 1))\n",
    "        # YOUR CODE HERE\n",
    "        # Process the title.\n",
    "        # title = ...\n",
    "\n",
    "        full_beg = self.full_emb(input2).permute((0, 2, 1))\n",
    "        # YOUR CODE HERE\n",
    "        # Process the description.\n",
    "        # desc = ...\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Process the categorical features.\n",
    "        # category = ...\n",
    "\n",
    "        concatenated = torch.cat(\n",
    "            [\n",
    "                title.view(title.size(0), -1),\n",
    "                desc.view(desc.size(0), -1),\n",
    "                category.view(category.size(0), -1),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Process the concatenated features to generate network output.\n",
    "        # out = ...\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAigoulWpHfe"
   },
   "source": [
    "### Bonus area 2: comparing RNN to CNN\n",
    "Try implementing simple RNN (or LSTM) and applying it to this task. Compare the quality/performance of these networks. \n",
    "*Hint: try to build networks with ~same number of paremeters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQVzJyCMpHfe"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPn2Q0wXpHfe"
   },
   "source": [
    "### Bonus area 3: fixing the data leaks\n",
    "Fix the data leak we ignored in the beginning of the __Deep Learning part__. Compare results with and without data leaks using same architectures and training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVgBWPpjpHfe"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-iOq02ZpHfe"
   },
   "source": [
    "__Terrible start-up idea #1962:__ make a tool that automaticaly rephrases your job description (or CV) to meet salary expectations :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "binpord_practice_cnn_for_texts_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
